{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, Trainer, TrainingArguments, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL)\n",
    "TRAIN_DATA ='../Data/train.csv'\n",
    "EVAL_DATA = '../Data/validate.csv'\n",
    "TEST_DATA = '../Data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>i would of got the 16gb iphone but i didnt have the extra $100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>just stay home and boring day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1</td>\n",
       "      <td>omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-1</td>\n",
       "      <td>is watching the green mile... does not want john coffey to die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1</td>\n",
       "      <td>am i one of the 1st 100 to tweet it? i'd really like to demo 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  \\\n",
       "0         -1   \n",
       "1         -1   \n",
       "2         -1   \n",
       "3          1   \n",
       "4          1   \n",
       "...      ...   \n",
       "9995       1   \n",
       "9996      -1   \n",
       "9997      -1   \n",
       "9998       1   \n",
       "9999       1   \n",
       "\n",
       "                                                                                                                                     text  \n",
       "0                             my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.   \n",
       "1                                                                         i would of got the 16gb iphone but i didnt have the extra $100   \n",
       "2                                                                                                          just stay home and boring day   \n",
       "3      been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...   \n",
       "4                   ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out   \n",
       "...                                                                                                                                   ...  \n",
       "9995      my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.   \n",
       "9996                              omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots   \n",
       "9997                                                                      is watching the green mile... does not want john coffey to die   \n",
       "9998                                                                    am i one of the 1st 100 to tweet it? i'd really like to demo 2.0   \n",
       "9999  it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!   \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train = pd.read_csv(TRAIN_DATA, encoding='latin')\n",
    "df_eval = pd.read_csv(EVAL_DATA, encoding='latin')\n",
    "df_test = pd.read_csv(TEST_DATA, encoding='latin')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i would of got the 16gb iphone but i didnt have the extra $100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>just stay home and boring day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>is watching the green mile... does not want john coffey to die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1</td>\n",
       "      <td>am i one of the 1st 100 to tweet it? i'd really like to demo 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0         0   \n",
       "1         0   \n",
       "2         0   \n",
       "3         1   \n",
       "4         1   \n",
       "...     ...   \n",
       "9995      1   \n",
       "9996      0   \n",
       "9997      0   \n",
       "9998      1   \n",
       "9999      1   \n",
       "\n",
       "                                                                                                                                     text  \n",
       "0                             my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.   \n",
       "1                                                                         i would of got the 16gb iphone but i didnt have the extra $100   \n",
       "2                                                                                                          just stay home and boring day   \n",
       "3      been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...   \n",
       "4                   ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out   \n",
       "...                                                                                                                                   ...  \n",
       "9995      my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.   \n",
       "9996                              omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots   \n",
       "9997                                                                      is watching the green mile... does not want john coffey to die   \n",
       "9998                                                                    am i one of the 1st 100 to tweet it? i'd really like to demo 2.0   \n",
       "9999  it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!   \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_preprocess(df):\n",
    "    df['target'].replace({-1:0},inplace=True)\n",
    "    df.rename(columns={'target':'label'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = data_preprocess(df_train)\n",
    "df_eval = data_preprocess(df_eval)\n",
    "df_test = data_preprocess(df_test)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5014\n",
      "1    4986\n",
      "Name: label, dtype: int64\n",
      "0    1012\n",
      "1     988\n",
      "Name: label, dtype: int64\n",
      "0    2521\n",
      "1    2479\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['label'].value_counts())\n",
    "print(df_eval['label'].value_counts())\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'my roomie called to inform me someone tried to break into our apartment when she was there today..awesome. '}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_eval = Dataset.from_pandas(df_eval)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811dcf8c3c4c42e28b3244426aff4252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fcea6ee0014ffab06de2974696d6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb4ae58e86449d09110f6eb60ae16a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset_train.map(tokenize_function, batched=True)\n",
    "eval_dataset = dataset_eval.map(tokenize_function, batched=True)\n",
    "test_dataset = dataset_test.map(tokenize_function, batched=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e4043bfccc4221a8cb08f2a421c29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5857, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5268, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93cae931d064f5091106ba1146a4b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5374781489372253, 'eval_accuracy': 0.8095, 'eval_f1': 0.8212106992022525, 'eval_precision': 0.7655293088363955, 'eval_recall': 0.8856275303643725, 'eval_runtime': 20.4364, 'eval_samples_per_second': 97.864, 'eval_steps_per_second': 12.233, 'epoch': 1.0}\n",
      "{'loss': 0.4431, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4138, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3957, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8563e38a7354a3da79a75705bf3c328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5077861547470093, 'eval_accuracy': 0.8225, 'eval_f1': 0.8059048660470202, 'eval_precision': 0.8763376932223543, 'eval_recall': 0.7459514170040485, 'eval_runtime': 20.392, 'eval_samples_per_second': 98.078, 'eval_steps_per_second': 12.26, 'epoch': 2.0}\n",
      "{'loss': 0.3036, 'learning_rate': 1e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2939, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3667e2a5334aabbb0898753bc267b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6526879668235779, 'eval_accuracy': 0.8275, 'eval_f1': 0.8244274809160305, 'eval_precision': 0.8290685772773797, 'eval_recall': 0.819838056680162, 'eval_runtime': 20.5543, 'eval_samples_per_second': 97.303, 'eval_steps_per_second': 12.163, 'epoch': 3.0}\n",
      "{'train_runtime': 988.0309, 'train_samples_per_second': 30.363, 'train_steps_per_second': 3.795, 'train_loss': 0.4157192626953125, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.4157192626953125, metrics={'train_runtime': 988.0309, 'train_samples_per_second': 30.363, 'train_steps_per_second': 3.795, 'train_loss': 0.4157192626953125, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\")\n",
    "# Default parameters of learning_rate = 0.00005 and per_device_train_batch_size = 8\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3907e8ca0eb148b2ae9eb4e988ad1036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = pd.Series(preds).map({0:'negative',1:'positive'})\n",
    "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.980419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.974968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.993825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.977494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.987779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.990140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.938397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred     label     score\n",
       "0        0  negative  0.980419\n",
       "1        1  positive  0.990340\n",
       "2        1  positive  0.990889\n",
       "3        0  negative  0.974968\n",
       "4        0  negative  0.993825\n",
       "...    ...       ...       ...\n",
       "4995     1  positive  0.985679\n",
       "4996     1  positive  0.977494\n",
       "4997     1  positive  0.987779\n",
       "4998     0  negative  0.990140\n",
       "4999     1  positive  0.938397\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(preds,labels,scores)), columns=['pred','label','score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = preds\n",
    "y_true = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8362    0.8564    0.8462      2521\n",
      "    positive     0.8503    0.8294    0.8397      2479\n",
      "\n",
      "    accuracy                         0.8430      5000\n",
      "   macro avg     0.8432    0.8429    0.8429      5000\n",
      "weighted avg     0.8432    0.8430    0.8430      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e91b13f5949daa2380171bd8be8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7049, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.4}\n",
      "{'loss': 0.702, 'learning_rate': 7.333333333333333e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98241d3118fd481c942abfd7285b3d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7113222479820251, 'eval_accuracy': 0.494, 'eval_f1': 0.6613119143239625, 'eval_precision': 0.494, 'eval_recall': 1.0, 'eval_runtime': 20.376, 'eval_samples_per_second': 98.155, 'eval_steps_per_second': 12.269, 'epoch': 1.0}\n",
      "{'loss': 0.6981, 'learning_rate': 6e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6968, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6972, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d167e8f51264987b1cae0dafddfd9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6961330771446228, 'eval_accuracy': 0.506, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 20.5924, 'eval_samples_per_second': 97.123, 'eval_steps_per_second': 12.14, 'epoch': 2.0}\n",
      "{'loss': 0.6967, 'learning_rate': 2e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6953, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac677c025d747b29df1c14b234dcbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6931102275848389, 'eval_accuracy': 0.506, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 20.1896, 'eval_samples_per_second': 99.061, 'eval_steps_per_second': 12.383, 'epoch': 3.0}\n",
      "{'train_runtime': 980.3228, 'train_samples_per_second': 30.602, 'train_steps_per_second': 3.825, 'train_loss': 0.6983877278645834, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7cd46d07494772971607308ad3f4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5042    1.0000    0.6704      2521\n",
      "    positive     0.0000    0.0000    0.0000      2479\n",
      "\n",
      "    accuracy                         0.5042      5000\n",
      "   macro avg     0.2521    0.5000    0.3352      5000\n",
      "weighted avg     0.2542    0.5042    0.3380      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef34338d5a04a1c8b3356bcdc6f9600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5159, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.4}\n",
      "{'loss': 0.4465, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e69176bb9984f58ba4014d8e2577d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44275006651878357, 'eval_accuracy': 0.829, 'eval_f1': 0.8388312912346844, 'eval_precision': 0.7848324514991182, 'eval_recall': 0.9008097165991903, 'eval_runtime': 19.9, 'eval_samples_per_second': 100.503, 'eval_steps_per_second': 12.563, 'epoch': 1.0}\n",
      "{'loss': 0.3736, 'learning_rate': 6e-06, 'epoch': 1.2}\n",
      "{'loss': 0.3674, 'learning_rate': 4.666666666666667e-06, 'epoch': 1.6}\n",
      "{'loss': 0.3592, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b89dbcec82e490fb792677156157183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44951704144477844, 'eval_accuracy': 0.8495, 'eval_f1': 0.8413284132841329, 'eval_precision': 0.8778877887788779, 'eval_recall': 0.8076923076923077, 'eval_runtime': 19.96, 'eval_samples_per_second': 100.2, 'eval_steps_per_second': 12.525, 'epoch': 2.0}\n",
      "{'loss': 0.289, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.4}\n",
      "{'loss': 0.3173, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c304dc7ccf44c97a0fed4544c656502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6046129465103149, 'eval_accuracy': 0.8505, 'eval_f1': 0.8496732026143792, 'eval_precision': 0.8441558441558441, 'eval_recall': 0.8552631578947368, 'eval_runtime': 20.2137, 'eval_samples_per_second': 98.943, 'eval_steps_per_second': 12.368, 'epoch': 3.0}\n",
      "{'train_runtime': 966.5447, 'train_samples_per_second': 31.038, 'train_steps_per_second': 3.88, 'train_loss': 0.3779412373860677, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a986b2725d62450d9435b9a1a55e792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8643    0.8592    0.8617      2521\n",
      "    positive     0.8577    0.8628    0.8602      2479\n",
      "\n",
      "    accuracy                         0.8610      5000\n",
      "   macro avg     0.8610    0.8610    0.8610      5000\n",
      "weighted avg     0.8610    0.8610    0.8610      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a37b0ad2fd8488b8aef1da593e005ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7154, 'learning_rate': 9.000199960007999e-05, 'epoch': 0.3}\n",
      "{'loss': 0.7077, 'learning_rate': 8.000399920015998e-05, 'epoch': 0.6}\n",
      "{'loss': 0.703, 'learning_rate': 7.000599880023996e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda94b8e2b0240289f292aceafd6ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7070051431655884, 'eval_accuracy': 0.494, 'eval_f1': 0.6613119143239625, 'eval_precision': 0.494, 'eval_recall': 1.0, 'eval_runtime': 19.8872, 'eval_samples_per_second': 100.567, 'eval_steps_per_second': 12.571, 'epoch': 1.0}\n",
      "{'loss': 0.7003, 'learning_rate': 6.000799840031994e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6984, 'learning_rate': 5.000999800039993e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6964, 'learning_rate': 4.0011997600479906e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da2291b3dce418184993e9da72fe041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6999432444572449, 'eval_accuracy': 0.506, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 20.1018, 'eval_samples_per_second': 99.493, 'eval_steps_per_second': 12.437, 'epoch': 2.0}\n",
      "{'loss': 0.699, 'learning_rate': 3.001399720055989e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6975, 'learning_rate': 2.001599680063987e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6958, 'learning_rate': 1.0017996400719856e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6934, 'learning_rate': 1.9996000799840033e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ba02e0be0b4f7eb7bcead1e4f8aeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6931371092796326, 'eval_accuracy': 0.506, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 19.8476, 'eval_samples_per_second': 100.768, 'eval_steps_per_second': 12.596, 'epoch': 3.0}\n",
      "{'train_runtime': 1012.4347, 'train_samples_per_second': 29.632, 'train_steps_per_second': 4.94, 'train_loss': 0.7007007583025288, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37fa975d124b5ba0caea2685350b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5042    1.0000    0.6704      2521\n",
      "    positive     0.0000    0.0000    0.0000      2479\n",
      "\n",
      "    accuracy                         0.5042      5000\n",
      "   macro avg     0.2521    0.5000    0.3352      5000\n",
      "weighted avg     0.2542    0.5042    0.3380      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98e92e15e3a4d5da37ef846a173f695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6155, 'learning_rate': 4.5000999800039995e-05, 'epoch': 0.3}\n",
      "{'loss': 0.5577, 'learning_rate': 4.000199960007999e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5314, 'learning_rate': 3.500299940011998e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42961bda658415eb2079fa1a16e4f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5472477078437805, 'eval_accuracy': 0.805, 'eval_f1': 0.8076923076923076, 'eval_precision': 0.7875, 'eval_recall': 0.8289473684210527, 'eval_runtime': 19.8557, 'eval_samples_per_second': 100.727, 'eval_steps_per_second': 12.591, 'epoch': 1.0}\n",
      "{'loss': 0.4773, 'learning_rate': 3.000399920015997e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4925, 'learning_rate': 2.5004999000199963e-05, 'epoch': 1.5}\n",
      "{'loss': 0.4787, 'learning_rate': 2.0005998800239953e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aca15e073e4091b5e15b9a6a26e234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.568938136100769, 'eval_accuracy': 0.811, 'eval_f1': 0.7936681222707425, 'eval_precision': 0.8613744075829384, 'eval_recall': 0.7358299595141701, 'eval_runtime': 19.9863, 'eval_samples_per_second': 100.068, 'eval_steps_per_second': 12.509, 'epoch': 2.0}\n",
      "{'loss': 0.4372, 'learning_rate': 1.5006998600279946e-05, 'epoch': 2.1}\n",
      "{'loss': 0.3639, 'learning_rate': 1.0007998400319935e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3443, 'learning_rate': 5.008998200359928e-06, 'epoch': 2.7}\n",
      "{'loss': 0.3547, 'learning_rate': 9.998000399920016e-09, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7743323538456b87c04ff35fd72d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6158543229103088, 'eval_accuracy': 0.828, 'eval_f1': 0.8274824473420261, 'eval_precision': 0.820079522862823, 'eval_recall': 0.8350202429149798, 'eval_runtime': 19.9636, 'eval_samples_per_second': 100.182, 'eval_steps_per_second': 12.523, 'epoch': 3.0}\n",
      "{'train_runtime': 1006.3097, 'train_samples_per_second': 29.812, 'train_steps_per_second': 4.97, 'train_loss': 0.46523013405302205, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2357876017a4bf38a4a3f208ece5cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8371    0.8136    0.8252      2521\n",
      "    positive     0.8157    0.8390    0.8272      2479\n",
      "\n",
      "    accuracy                         0.8262      5000\n",
      "   macro avg     0.8264    0.8263    0.8262      5000\n",
      "weighted avg     0.8265    0.8262    0.8262      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.00005, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6365ed123f04160a55cb15dd4245d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5459, 'learning_rate': 9.000199960007999e-06, 'epoch': 0.3}\n",
      "{'loss': 0.4827, 'learning_rate': 8.000399920015997e-06, 'epoch': 0.6}\n",
      "{'loss': 0.4866, 'learning_rate': 7.000599880023996e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8686aa41989d4cb2bd8761325e8ad2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5203272700309753, 'eval_accuracy': 0.8325, 'eval_f1': 0.8363458720078164, 'eval_precision': 0.8083097261567517, 'eval_recall': 0.8663967611336032, 'eval_runtime': 20.0675, 'eval_samples_per_second': 99.663, 'eval_steps_per_second': 12.458, 'epoch': 1.0}\n",
      "{'loss': 0.4255, 'learning_rate': 6.000799840031995e-06, 'epoch': 1.2}\n",
      "{'loss': 0.4524, 'learning_rate': 5.000999800039993e-06, 'epoch': 1.5}\n",
      "{'loss': 0.4333, 'learning_rate': 4.001199760047991e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0028952fa534751867855e4242ac34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6014479398727417, 'eval_accuracy': 0.8425, 'eval_f1': 0.8368720870015536, 'eval_precision': 0.8568398727465536, 'eval_recall': 0.8178137651821862, 'eval_runtime': 19.8515, 'eval_samples_per_second': 100.748, 'eval_steps_per_second': 12.594, 'epoch': 2.0}\n",
      "{'loss': 0.3889, 'learning_rate': 3.0013997200559893e-06, 'epoch': 2.1}\n",
      "{'loss': 0.3546, 'learning_rate': 2.001599680063987e-06, 'epoch': 2.4}\n",
      "{'loss': 0.3699, 'learning_rate': 1.0017996400719856e-06, 'epoch': 2.7}\n",
      "{'loss': 0.3523, 'learning_rate': 1.999600079984003e-09, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8422125c7b0b44e58b9f9b0844125459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6682096123695374, 'eval_accuracy': 0.848, 'eval_f1': 0.8476953907815633, 'eval_precision': 0.8392857142857143, 'eval_recall': 0.8562753036437247, 'eval_runtime': 19.8547, 'eval_samples_per_second': 100.732, 'eval_steps_per_second': 12.591, 'epoch': 3.0}\n",
      "{'train_runtime': 1011.6386, 'train_samples_per_second': 29.655, 'train_steps_per_second': 4.943, 'train_loss': 0.4291056510650621, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f446e4970dad4dc0afc4a63d3489dca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8640    0.8544    0.8592      2521\n",
      "    positive     0.8536    0.8633    0.8584      2479\n",
      "\n",
      "    accuracy                         0.8588      5000\n",
      "   macro avg     0.8588    0.8588    0.8588      5000\n",
      "weighted avg     0.8589    0.8588    0.8588      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c65657f325941c9bd5a50ff03db72f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7063, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6994, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29fd28ac94e486ea564a421cc4e4ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7308576107025146, 'eval_accuracy': 0.494, 'eval_f1': 0.6613119143239625, 'eval_precision': 0.494, 'eval_recall': 1.0, 'eval_runtime': 32.9701, 'eval_samples_per_second': 60.661, 'eval_steps_per_second': 7.583, 'epoch': 1.0}\n",
      "{'loss': 0.6973, 'learning_rate': 5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6954, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53af4590e9740f0bc491c2457601981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6999887824058533, 'eval_accuracy': 0.506, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 24.4965, 'eval_samples_per_second': 81.644, 'eval_steps_per_second': 10.206, 'epoch': 2.0}\n",
      "{'loss': 0.6967, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6947, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb93a5de96414be7a0bec355de1286f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6935511231422424, 'eval_accuracy': 0.494, 'eval_f1': 0.6613119143239625, 'eval_precision': 0.494, 'eval_recall': 1.0, 'eval_runtime': 24.5111, 'eval_samples_per_second': 81.596, 'eval_steps_per_second': 10.199, 'epoch': 3.0}\n",
      "{'train_runtime': 1302.4506, 'train_samples_per_second': 23.034, 'train_steps_per_second': 2.303, 'train_loss': 0.6983077392578125, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d62ba86c1543069cc92af47d2d3e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.0000    0.0000    0.0000      2521\n",
      "    positive     0.4958    1.0000    0.6629      2479\n",
      "\n",
      "    accuracy                         0.4958      5000\n",
      "   macro avg     0.2479    0.5000    0.3315      5000\n",
      "weighted avg     0.2458    0.4958    0.3287      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f3d3e8c14c4e10957790b0065283f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5648, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5036, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64bc2ac6a634d89b30e0c3521d30f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6612009406089783, 'eval_accuracy': 0.778, 'eval_f1': 0.7881679389312978, 'eval_precision': 0.7454873646209387, 'eval_recall': 0.8360323886639676, 'eval_runtime': 32.0559, 'eval_samples_per_second': 62.391, 'eval_steps_per_second': 7.799, 'epoch': 1.0}\n",
      "{'loss': 0.4193, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.384, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08e3c90377248048b90348323da4916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.447526216506958, 'eval_accuracy': 0.8285, 'eval_f1': 0.8191881918819188, 'eval_precision': 0.8547854785478548, 'eval_recall': 0.7864372469635628, 'eval_runtime': 32.2024, 'eval_samples_per_second': 62.107, 'eval_steps_per_second': 7.763, 'epoch': 2.0}\n",
      "{'loss': 0.2817, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.2883, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f07d45d70f1477aaafdd29ee96f6f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5473266839981079, 'eval_accuracy': 0.8335, 'eval_f1': 0.8340807174887892, 'eval_precision': 0.8213935230618253, 'eval_recall': 0.847165991902834, 'eval_runtime': 34.5493, 'eval_samples_per_second': 57.888, 'eval_steps_per_second': 7.236, 'epoch': 3.0}\n",
      "{'train_runtime': 1501.4088, 'train_samples_per_second': 19.981, 'train_steps_per_second': 1.998, 'train_loss': 0.4069336853027344, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7e4dc0f3474e2086c403a8a2581e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8533    0.8282    0.8406      2521\n",
      "    positive     0.8304    0.8552    0.8426      2479\n",
      "\n",
      "    accuracy                         0.8416      5000\n",
      "   macro avg     0.8418    0.8417    0.8416      5000\n",
      "weighted avg     0.8419    0.8416    0.8416      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.00005, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e803e9e97094c89b4e34e73a4f0b697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5219, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.5}\n",
      "{'loss': 0.4067, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c51081f1c4494680a9a48b15d30a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4011552035808563, 'eval_accuracy': 0.8365, 'eval_f1': 0.8380386329866271, 'eval_precision': 0.8205625606207565, 'eval_recall': 0.8562753036437247, 'eval_runtime': 39.3018, 'eval_samples_per_second': 50.888, 'eval_steps_per_second': 6.361, 'epoch': 1.0}\n",
      "{'loss': 0.3413, 'learning_rate': 5e-06, 'epoch': 1.5}\n",
      "{'loss': 0.3321, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb6572e6d09480bb90650d14cab6a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4830947816371918, 'eval_accuracy': 0.8475, 'eval_f1': 0.8447837150127226, 'eval_precision': 0.849539406345957, 'eval_recall': 0.840080971659919, 'eval_runtime': 37.4334, 'eval_samples_per_second': 53.428, 'eval_steps_per_second': 6.679, 'epoch': 2.0}\n",
      "{'loss': 0.2859, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3218, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8fa6c300984174877fcf2487daf636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5714234113693237, 'eval_accuracy': 0.8515, 'eval_f1': 0.8517224163754369, 'eval_precision': 0.8403940886699507, 'eval_recall': 0.8633603238866396, 'eval_runtime': 38.7315, 'eval_samples_per_second': 51.637, 'eval_steps_per_second': 6.455, 'epoch': 3.0}\n",
      "{'train_runtime': 1596.4043, 'train_samples_per_second': 18.792, 'train_steps_per_second': 1.879, 'train_loss': 0.3682825419108073, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af5c39af33049648de1b7901b579d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8648    0.8473    0.8559      2521\n",
      "    positive     0.8478    0.8653    0.8565      2479\n",
      "\n",
      "    accuracy                         0.8562      5000\n",
      "   macro avg     0.8563    0.8563    0.8562      5000\n",
      "weighted avg     0.8564    0.8562    0.8562      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Batch Size  F1 Score\n",
       "0        0.00010           6    0.3352\n",
       "1        0.00005           6    0.8262\n",
       "2        0.00001           6    0.8588\n",
       "3        0.00010           8    0.3352\n",
       "4        0.00005           8    0.8429\n",
       "5        0.00001           8    0.8610\n",
       "6        0.00010          10    0.3315\n",
       "7        0.00005          10    0.8416\n",
       "8        0.00001          10    0.8562"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_results = [[0.0001, 6,  0.3352],[0.00005, 6,  0.8262],[0.00001, 6,  0.8588],[0.0001, 8,  0.3352],[0.00005, 8,  0.8429],[0.00001, 8,  0.861],[0.0001, 10,  0.3315],[0.00005, 10,  0.8416],[0.00001, 10,  0.8562]]\n",
    "df_results = pd.DataFrame(f1_results, columns = ['Learning Rate', 'Batch Size', 'F1 Score'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
