{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, Trainer, TrainingArguments, RobertaForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL)\n",
    "TRAIN_DATA ='../Data/train.csv'\n",
    "EVAL_DATA = '../Data/validate.csv'\n",
    "TEST_DATA = '../Data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>i would of got the 16gb iphone but i didnt have the extra $100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>just stay home and boring day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1</td>\n",
       "      <td>omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-1</td>\n",
       "      <td>is watching the green mile... does not want john coffey to die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1</td>\n",
       "      <td>am i one of the 1st 100 to tweet it? i'd really like to demo 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  \\\n",
       "0         -1   \n",
       "1         -1   \n",
       "2         -1   \n",
       "3          1   \n",
       "4          1   \n",
       "...      ...   \n",
       "9995       1   \n",
       "9996      -1   \n",
       "9997      -1   \n",
       "9998       1   \n",
       "9999       1   \n",
       "\n",
       "                                                                                                                                     text  \n",
       "0                             my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.   \n",
       "1                                                                         i would of got the 16gb iphone but i didnt have the extra $100   \n",
       "2                                                                                                          just stay home and boring day   \n",
       "3      been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...   \n",
       "4                   ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out   \n",
       "...                                                                                                                                   ...  \n",
       "9995      my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.   \n",
       "9996                              omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots   \n",
       "9997                                                                      is watching the green mile... does not want john coffey to die   \n",
       "9998                                                                    am i one of the 1st 100 to tweet it? i'd really like to demo 2.0   \n",
       "9999  it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!   \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train = pd.read_csv(TRAIN_DATA, encoding='latin')\n",
    "df_eval = pd.read_csv(EVAL_DATA, encoding='latin')\n",
    "df_test = pd.read_csv(TEST_DATA, encoding='latin')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i would of got the 16gb iphone but i didnt have the extra $100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>just stay home and boring day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>is watching the green mile... does not want john coffey to die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1</td>\n",
       "      <td>am i one of the 1st 100 to tweet it? i'd really like to demo 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0         0   \n",
       "1         0   \n",
       "2         0   \n",
       "3         1   \n",
       "4         1   \n",
       "...     ...   \n",
       "9995      1   \n",
       "9996      0   \n",
       "9997      0   \n",
       "9998      1   \n",
       "9999      1   \n",
       "\n",
       "                                                                                                                                     text  \n",
       "0                             my roomie called to inform me someone tried to break into our apartment when she was there today..awesome.   \n",
       "1                                                                         i would of got the 16gb iphone but i didnt have the extra $100   \n",
       "2                                                                                                          just stay home and boring day   \n",
       "3      been voting for the eu parliament and the heritage rules of the danish monarchy. then a spinning class. feel good about myself...   \n",
       "4                   ooooh sbs2!! that's exciting and relevant to my media audiences research into psbs - look forward to checking it out   \n",
       "...                                                                                                                                   ...  \n",
       "9995      my man has both sides..with me he's the sweetest... just dont f*ck with me cuz then theres troble.i'll take both sides please.   \n",
       "9996                              omg why is this weather so disgusting today???? looks like i'm going to have to pull out the rainboots   \n",
       "9997                                                                      is watching the green mile... does not want john coffey to die   \n",
       "9998                                                                    am i one of the 1st 100 to tweet it? i'd really like to demo 2.0   \n",
       "9999  it's a pity spymaster has to bring 'attacking' into it. 20th century thinking. would much rather support tweeple in positive ways!   \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_preprocess(df):\n",
    "    df['target'].replace({-1:0},inplace=True)\n",
    "    df.rename(columns={'target':'label'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = data_preprocess(df_train)\n",
    "df_eval = data_preprocess(df_eval)\n",
    "df_test = data_preprocess(df_test)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5014\n",
      "1    4986\n",
      "Name: label, dtype: int64\n",
      "0    1012\n",
      "1     988\n",
      "Name: label, dtype: int64\n",
      "0    2521\n",
      "1    2479\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['label'].value_counts())\n",
    "print(df_eval['label'].value_counts())\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'my roomie called to inform me someone tried to break into our apartment when she was there today..awesome. '}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_eval = Dataset.from_pandas(df_eval)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35524cfa12e3459f9d21023b22182634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a954da0c9c34d5ba2e23b327dc62f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2bd1852e0746629edf6579e79ae2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset_train.map(tokenize_function, batched=True)\n",
    "eval_dataset = dataset_eval.map(tokenize_function, batched=True)\n",
    "test_dataset = dataset_test.map(tokenize_function, batched=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a4b01b94b04a05976b54c6eaf4011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6128, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5213, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d853415e5214bb4bac9c024a0467a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47290724515914917, 'eval_accuracy': 0.808, 'eval_f1': 0.8102766798418972, 'eval_precision': 0.7915057915057915, 'eval_recall': 0.8299595141700404, 'eval_runtime': 20.8341, 'eval_samples_per_second': 95.996, 'eval_steps_per_second': 12.0, 'epoch': 1.0}\n",
      "{'loss': 0.4466, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4322, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4301, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02679654c4c14fb6b9354e05562236e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5027541518211365, 'eval_accuracy': 0.8325, 'eval_f1': 0.8261546445251687, 'eval_precision': 0.8477103301384451, 'eval_recall': 0.805668016194332, 'eval_runtime': 20.4716, 'eval_samples_per_second': 97.696, 'eval_steps_per_second': 12.212, 'epoch': 2.0}\n",
      "{'loss': 0.3274, 'learning_rate': 1e-05, 'epoch': 2.4}\n",
      "{'loss': 0.342, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4277d2f91db4c71a1db6f5fe740655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5597289204597473, 'eval_accuracy': 0.8395, 'eval_f1': 0.8384499245093106, 'eval_precision': 0.8338338338338338, 'eval_recall': 0.8431174089068826, 'eval_runtime': 20.2336, 'eval_samples_per_second': 98.845, 'eval_steps_per_second': 12.356, 'epoch': 3.0}\n",
      "{'train_runtime': 988.0571, 'train_samples_per_second': 30.363, 'train_steps_per_second': 3.795, 'train_loss': 0.43804378458658855, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.43804378458658855, metrics={'train_runtime': 988.0571, 'train_samples_per_second': 30.363, 'train_steps_per_second': 3.795, 'train_loss': 0.43804378458658855, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9506c6fa888c457a8a497264e98ec4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = pd.Series(preds).map({0:'negative',1:'positive'})\n",
    "scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.780179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.988955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.989522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.954995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.980910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.986783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.989224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.962709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.978101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.981090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred     label     score\n",
       "0        0  negative  0.780179\n",
       "1        1  positive  0.988955\n",
       "2        1  positive  0.989522\n",
       "3        0  negative  0.954995\n",
       "4        0  negative  0.980910\n",
       "...    ...       ...       ...\n",
       "4995     1  positive  0.986783\n",
       "4996     1  positive  0.989224\n",
       "4997     1  positive  0.962709\n",
       "4998     0  negative  0.978101\n",
       "4999     1  positive  0.981090\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(preds,labels,scores)), columns=['pred','label','score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = preds\n",
    "y_true = test_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.84      0.84      2521\n",
      "    positive       0.84      0.85      0.84      2479\n",
      "\n",
      "    accuracy                           0.84      5000\n",
      "   macro avg       0.84      0.84      0.84      5000\n",
      "weighted avg       0.84      0.84      0.84      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28dc92046949a8b2e6cdcdeabad652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6128, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5213, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f8be82ac054ca3b104135bd0f455f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47290724515914917, 'eval_accuracy': 0.808, 'eval_f1': 0.8102766798418972, 'eval_precision': 0.7915057915057915, 'eval_recall': 0.8299595141700404, 'eval_runtime': 20.2973, 'eval_samples_per_second': 98.535, 'eval_steps_per_second': 12.317, 'epoch': 1.0}\n",
      "{'loss': 0.4466, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4322, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4301, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4923ea464c415b9c9613b92398e033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5027541518211365, 'eval_accuracy': 0.8325, 'eval_f1': 0.8261546445251687, 'eval_precision': 0.8477103301384451, 'eval_recall': 0.805668016194332, 'eval_runtime': 20.4844, 'eval_samples_per_second': 97.635, 'eval_steps_per_second': 12.204, 'epoch': 2.0}\n",
      "{'loss': 0.3274, 'learning_rate': 1e-05, 'epoch': 2.4}\n",
      "{'loss': 0.342, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b2b72da375454f8eb7284e652c0a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5597289204597473, 'eval_accuracy': 0.8395, 'eval_f1': 0.8384499245093106, 'eval_precision': 0.8338338338338338, 'eval_recall': 0.8431174089068826, 'eval_runtime': 20.7411, 'eval_samples_per_second': 96.427, 'eval_steps_per_second': 12.053, 'epoch': 3.0}\n",
      "{'train_runtime': 978.0621, 'train_samples_per_second': 30.673, 'train_steps_per_second': 3.834, 'train_loss': 0.43804378458658855, 'epoch': 3.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67      2521\n",
      "    positive       0.00      0.00      0.00      2479\n",
      "\n",
      "    accuracy                           0.50      5000\n",
      "   macro avg       0.25      0.50      0.34      5000\n",
      "weighted avg       0.25      0.50      0.34      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00005, per_device_train_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00005, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.0001, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\cjlongoria\\Documents\\College\\CS522\\Project\\.venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b80dfdc4b54f33bb1491bcdf219ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5824, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5006, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c144fb7432841209a17448472d95d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5520062446594238, 'eval_accuracy': 0.804, 'eval_f1': 0.8140417457305503, 'eval_precision': 0.7660714285714286, 'eval_recall': 0.868421052631579, 'eval_runtime': 20.8512, 'eval_samples_per_second': 95.918, 'eval_steps_per_second': 11.99, 'epoch': 1.0}\n",
      "{'loss': 0.4286, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3769, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79492b85facf455b8b967a3700efc8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4454420804977417, 'eval_accuracy': 0.827, 'eval_f1': 0.8214654282765737, 'eval_precision': 0.8378947368421052, 'eval_recall': 0.805668016194332, 'eval_runtime': 20.3518, 'eval_samples_per_second': 98.271, 'eval_steps_per_second': 12.284, 'epoch': 2.0}\n",
      "{'loss': 0.294, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3045, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369388ade80b4a9583e49e48def4bccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.542754054069519, 'eval_accuracy': 0.8345, 'eval_f1': 0.8298200514138818, 'eval_precision': 0.8432601880877743, 'eval_recall': 0.8168016194331984, 'eval_runtime': 20.5197, 'eval_samples_per_second': 97.467, 'eval_steps_per_second': 12.183, 'epoch': 3.0}\n",
      "{'train_runtime': 982.8651, 'train_samples_per_second': 30.523, 'train_steps_per_second': 3.052, 'train_loss': 0.4145046234130859, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bfff6b0ca0424ab32ec626885b709b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.85      0.84      2521\n",
      "    positive       0.84      0.82      0.83      2479\n",
      "\n",
      "    accuracy                           0.84      5000\n",
      "   macro avg       0.84      0.84      0.84      5000\n",
      "weighted avg       0.84      0.84      0.84      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00005, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=0.00001, per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = test_dataset['label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
